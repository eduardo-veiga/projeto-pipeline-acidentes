# Define a base comum para os serviços do Airflow
x-airflow-common: &x-airflow-common
  build: .
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres-airflow:5432/airflow
    - AIRFLOW__CORE__LOAD_EXAMPLES=False
    - AIRFLOW__WEBSERVER__SECRET_KEY=sua-chave-secreta-aqui
    - AIRFLOW_UID=50000
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - ./spark_jobs:/opt/spark/jobs
  networks:
    - data-pipeline-net
  depends_on:
    - postgres-airflow

services:
  # --- 1. & 5. Orquestração (Airflow) ---
  
  postgres-airflow:
    image: postgres:13
    container_name: postgres-airflow
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - airflow_pg_data:/var/lib/postgresql/data
    networks:
      - data-pipeline-net

  airflow-init:
    <<: *x-airflow-common
    container_name: airflow-init
    command: >
      bash -c "
        airflow db upgrade &&
        airflow users create --role Admin --username admin --email admin@example.com --firstname Admin --lastname User --password admin
      "
    
  airflow-webserver:
    <<: *x-airflow-common
    container_name: airflow-webserver
    ports:
      - "8080:8080" # ⬅️ ACESSE AQUI O AIRFLOW
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    <<: *x-airflow-common
    container_name: airflow-scheduler
    command: scheduler

  # --- 2. Data Lake (MinIO) ---
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"  # API
      - "9001:9001"  # ⬅️ ACESSE AQUI O MINIO (CONSOLE)
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    networks:
      - data-pipeline-net

  # --- 1. Ingestão (NiFi) ---
  nifi:
    image: apache/nifi:1.23.2
    container_name: nifi
    ports:
      - "8081:8080" # ⬅️ ACESSE AQUI O NIFI (em :8081)
    environment:
      - NIFI_WEB_HTTP_PORT=8080
      - NIFI_SENSITIVE_PROPS_KEY=minha_chave_secreta_123
    volumes:
      - nifi_data:/opt/nifi/nifi-current/conf
      - nifi_flowfile:/opt/nifi/nifi-current/flowfile_repository
      - ./landing_zone:/opt/nifi/landing_zone
    networks:
      - data-pipeline-net
    
  # --- 3. Processamento (Spark) ---
  spark-master:
    image: bitnamilegacy/spark # <-- CORREÇÃO AQUI (sem tag)
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    ports:
      - "8082:8080" # ⬅️ ACESSE AQUI O SPARK (UI)
      - "7077:7077"
    volumes:
      - ./spark_jobs:/opt/spark/jobs
    networks:
      - data-pipeline-net

  spark-worker:
    image: bitnamilegacy/spark # <-- CORREÇÃO AQUI (sem tag)
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - ./spark_jobs:/opt/spark/jobs
    networks:
      - data-pipeline-net

  # --- 4. Data Warehouse (PostgreSQL) ---
  postgres-dw:
    image: postgres:13
    container_name: postgres-dw
    ports:
      - "5432:5432" # ⬅️ O POWER BI VAI CONECTAR AQUI
    environment:
      - POSTGRES_USER=admin_dw
      - POSTGRES_PASSWORD=admin_dw
      - POSTGRES_DB=acidentes_dw
    volumes:
      - postgres_dw_data:/var/lib/postgresql/data
    networks:
      - data-pipeline-net

# Define a rede e os volumes
networks:
  data-pipeline-net:
    driver: bridge

volumes:
  airflow_pg_data:
  minio_data:
  postgres_dw_data:
  nifi_data:
  nifi_flowfile:

